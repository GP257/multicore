{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533203a6",
   "metadata": {},
   "source": [
    "# GEOPHYS 257 (Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)\n",
    "\n",
    "## Multicore Parallelization of Matrix Multiplication Using Numba\n",
    "\n",
    "In this lab we will be using Numba to accelerate matrix-matrix multiplications by exploiting parallelism. Even most laptops today have Multicore CPUs, where a *core* is a microprocessor, and each core is usually a copy of the each other core. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba](https://numba.readthedocs.io/en/stable/index.html): Documentation\n",
    "* [Numba in 30 min](https://youtu.be/DPyPmoeUdcE): Conference presentation video\n",
    "\n",
    "\n",
    "## Required Preperation\n",
    "Please watch the following videos before starting the lab (each is pretty short):\n",
    "* [Introduction to Parallel Computing](https://youtu.be/RNVIcm8-6RE)\n",
    "* [Amdahl's Law](https://youtu.be/Axx2xuB-Xuo)\n",
    "* [CPU Caching](https://youtu.be/KmairurdiaY)\n",
    "* [Pipelining](https://youtu.be/zPmfprtdzCE)\n",
    "* [Instruction Level Parallelism](https://youtu.be/ZoDUI3gTkDI)\n",
    "* [Introduction to SIMD](https://youtu.be/o_n4AKwdfiA)\n",
    "\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "Please run the following cells.  Examine the result of the example function that makes use of the *timer* wrapper. Also please use this timer wrapper (defined below) for all the exercises that follow.\n",
    "\n",
    "#### Load python modules (Note: you may need to install some Python packages for the modules below, e.g. Numba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9957106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from time import time, sleep\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc2aac",
   "metadata": {},
   "source": [
    "#### Define a function-decorator for timing the runtime of your functions\n",
    "\n",
    "Below is some code that you can use to wrap your functions so that you can time them individually.  The function defined imeadiately below the *timer()* is an example of how to use the warpper. In the cell that follows, execute this example function. (Note, for this timer, were are making use of something called *decorators*, but a discussion about this feature is outside the scope of this class.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0a801",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# defining a function decorator for timing other functions\n",
    "def mytimer(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        t_start = time() # Students look here\n",
    "        result = func(*args, **kwargs)\n",
    "        t_end = time() # Students also look here. This is how you can time things inside functions/classes\n",
    "        print(f'Function: \\'{func.__name__}({kwargs})\\' executed in {(t_end-t_start):4.3f}s\\n')\n",
    "        return result\n",
    "    return wrap\n",
    "    \n",
    "\n",
    "# Example of how to use. NOTE the \"@mytimer\" stated just above the function definition\n",
    "@mytimer\n",
    "def example_sum_timer_wrap(N):\n",
    "    \"\"\" Sum the squares of the intergers, i, contained in [1-N] \"\"\"\n",
    "    return np.sum(np.arange(1,N+1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb7b5c",
   "metadata": {},
   "source": [
    "#### Run the example function\n",
    "\n",
    "Run the example function for each of the following instances of $N = 10^5, 10^6, 10^7, 10^8$. Please examine the results, in particular, how the runtime changes with respect to $N$.\n",
    "\n",
    "\n",
    "**Answer** the following questions in the markdown cell that follow the code. \n",
    "* For each factor-of-ten increase in $N$, roughly how much longer was the runtime of the function?\n",
    "* Does this slowdown in the runtime make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa466992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the example function above for each value of N (try making one-call first, then loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4c88",
   "metadata": {},
   "source": [
    "#### Your answer for Exerciese 0 questions here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8161fc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 1: Naive matrix multiplication: \n",
    "\n",
    "For this exercise, we will write our own matrx-matrix multiplication function.  We are goint to be naieve about it, so we want to loop over individual indices, instead of using slicing (in the next section, we will parallelize this code and we want to see how well Numba does at speeding up the naive code).\n",
    "\n",
    "#### Tasks for this exercise\n",
    "\n",
    "1. Write a function with that calculates matrix-matrix multiplication such that $C = A\\cdot B$, where $A$, $B$, and $C$ are 2D numpy arrays. Make the dtype for the $C$ matrix the same as the dtype for $A$. Below is a stencil you can start with. test your results for accuracy and performance against the np.dot() function. To time the np.dot() function, you can wrap it in another function and use the *mytimer* wrapper; however, please opy the code for testing the A and B dimensions into your function wrapper for the np.dot() function.  This keeps the runtime comparison as a more \"apples-to-apples\" like comparison.\n",
    "\n",
    "```python\n",
    "@mytimer\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in range(<val>):\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Define your functions in the cell below.\n",
    "3. Test and compare the accuracy and runtime of your functions in the next cell.\n",
    "    - Test with non-square Matrices: $A \\in \\mathbb{R}^{N\\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$. With $N = 64$, $K = 32$, and $M = 128$.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 64, 128,$ and $256$.\n",
    "    - Test the following three cases:\n",
    "        - Case-1. $A$ and $B$ **both** have dtype=np.**float32** (make sure that $C$ is also of dtype=np.**float32**)\n",
    "        - Case-2. $A$ has dtype=np.**float64**, but $B$ has dtype=np.**float32**\n",
    "        - Case-3. $A$ and $B$ **both** have dtype=np.**float64** (make sure that $C$ is also of dtype=np.**float64**)\n",
    "    - For all three case above:\n",
    "        - Calculate and show the error by computing the sum of the differnece between the $C$ matrices computed from numpy.dot() and your my_naive_matmul() functions. Assume that numpy.dot() is correct\n",
    "        - Calculate and show the *speedup* that the faster function has versus the slower function.\n",
    "        - Comment on the which of the three cases is fastest, and comment on what the speedup of the fastest case is and why it is the fastest case.\n",
    "4. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "A = np.random.rand(N, K)\n",
    "B = np.random.rand(\"for-you-to-figure-out\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 1 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6404b75",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 1 here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232378",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 1 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea011152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code here for runtime and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae756d65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 2: Using Numba to speed up matrix multiplication: \n",
    "\n",
    "For this exercise, numba to speedup our matrx-matrix multiplication function below. However, there is an interesting twist to this exercise. We will write six versions of the naive function you wrote above. One function for each of the six possible permutations of three loops used to calculate the multplication (i.e. ijk, ikj, jik, etc.)\n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Make six copies your code for the my_naive_matmul(), one copy for each of the possible permutations and define them in the cell bellow:\n",
    "    - One of these function will have the same loop order as the my_naive_matmul() function. \n",
    "    - However name these functions: numba_mul_\\<perm\\>(), where \\<perm\\> should be replaced by the specific loop order of the function.\n",
    "2. In the cell that follows your function definitions, test and compare the accuracy and runtime of your functions against the numpy.dot() function.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 128, 256,$ and $512$.\n",
    "    - For each matrix set their dtype as: dtype=np.float64\n",
    "    - Calculate and show the error between your functions and the numpy.dot() function. (Same as in Exercise 1.)\n",
    "    - Calculate and show the *speedup* that the fastest function has versus the all the other functions.\n",
    "    - You should notice that one of your permutation functions is faster than the others. For this case show the following:\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has versus the my_naive_matmul().\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has when $A$, $B$, and $C$ are all of dtype=np.float64 vs all are of dtype=np.float32.\n",
    "3. Create your matrices using random numbers. (Same as in Exercise 1.) \n",
    "4. For each function, you need to add a function decorator for numba. Numba will *JIT* the function (**J**ust **I**n **T**ime compilation). \n",
    "    - Use the flag to keep a \"cache\" of the compiled code (not to be confused with CPU cache). **Note**: to get accurate timings, you will need to run your tests **twice**, because during the first run, the code is compiled and this compile time will be included in your runtime. After the first run, the compiled binary will be stored, so consecutive runs will be faster.\n",
    "    - Use the flag to use *fast math*\n",
    "    - Use the flag to disable the Python Global Interpretor Lock (GIL).\n",
    "    - The code below should get you started, but it is incomplete.\n",
    "    \n",
    "```python   \n",
    "    \n",
    "@mytimer\n",
    "@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "def numba_mul_<perm>(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in numba.prange(<val>): # !!!! LOOK HERE !!!!\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "    \n",
    "6. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the questions asked in the markdown cell.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc104d3",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 2 here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327e73a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 2 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code here for runtimes, accuracy, and speedups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c61afc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Discussion for Exercise 2\n",
    "1. What do you think is causing the differences in performance between the various permutations?\n",
    "1. Which function is fastest (permutations or numpy.dot)? Why do you think this function is the fastest, and could there be multiple factors involved regarding the superior performance?\n",
    "1. When comparing the matrix-matrix performance between the cases were all matrices had dtype=np.float64 vs.  all matrices having dtype=np.float32, which dtype was fastest? Roughtly what was the speedup when using this dtype vs the other?\n",
    "1. Does Amdahl's Law play a major factor in the performance differences? Which part of the matrix-matrix multiplication was not parallelized (serial portion)? (Hint: which matricies did we reuse for each function?) Could we parallelize this part, and if so, are there caveats?\n",
    "1. Do you think all codes should be parallelized? What about matrix-matrix multiplication? (This is a subjective question, but I am looking for a brief, but rational and informed arguement).\n",
    "1. For those taking the class for **4 units**: Regarding the performance differences, which Law do you think is more relevant when comparing the performance differences between each of the functions in this exercise, Amdahl's Law or Gustafson's Law? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5170b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
